{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业2：纯天然手工制作神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作业中，你将体验到如何用最基本的矩阵运算来实现一个前馈神经网络。在该实现中，唯一需要使用到的软件包是Numpy。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第1题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用 Git 的基本操作，对第一次作业进行一次修订。具体要求如下：\n",
    "\n",
    "1. 将库的名称统一为 “ai2021s”。\n",
    "2. 在库的根目录创建文件夹 `HW1`，将第一次作业提交的 Notebook 文件重命名为 `numpy_practice.ipynb`，并移至 `HW1` 文件夹中。\n",
    "3. 提交这一修改，提交信息为 “reorganizing project files”。\n",
    "4. 从本次作业起，均按上述规则建立文件夹。本次作业的文件名为`fnn.ipynb`。\n",
    "5. 优化作业1中各函数的编写，直接对原始文件修改并利用 Git 命令提交。根据本次修改的结果，你将对作业1获得最多20分的加分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第2题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) 根据作业1的内容，编写**数值稳定**的函数 `sigmoid(x)` 和 `softplus(x)`，要求它适用于 Numpy 向量和矩阵，即当 `x` 是一维或二维数组时，返回一个相同大小和形状的数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid(v) =\n",
      " [0.25244196 0.73053634 0.57027629 0.18148857 0.35925474 0.83908511]\n",
      "Sigmoid(m) =\n",
      " [[0.08116076 0.39438602 0.78004631]\n",
      " [0.29593301 0.33650995 0.47634044]\n",
      " [0.81628676 0.34549479 0.39079256]\n",
      " [0.39308777 0.90078077 0.89905661]\n",
      " [0.73185488 0.59536432 0.67642017]]\n",
      "Softplus(v) =\n",
      " [0.29094333 1.31132175 0.84461281 0.20026791 0.44512331 1.82687967]\n",
      "Softplus(m) =\n",
      " [[0.08464411 0.50151249 1.51433825]\n",
      " [0.35088177 0.41024142 0.6469135 ]\n",
      " [1.69437919 0.42387573 0.49559644]\n",
      " [0.49937109 2.31042345 2.29319537]\n",
      " [1.31622694 0.90476816 1.12830942]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(linewidth=100) \n",
    "\n",
    "def sigmoid(x):\n",
    "    \n",
    "    res = np.zeros_like(x)\n",
    "    fenzi = np.where(x >= 0, 1, np.exp(x))\n",
    "    fenmu = 1 + np.exp(-abs(x))\n",
    "    res = fenzi/fenmu\n",
    "    \n",
    "    return res\n",
    "\n",
    "def softplus(x):\n",
    "    \n",
    "    res = np.zeros_like(x)\n",
    "    part1 = np.where(x >= 0, x, 0)\n",
    "    part2 = np.log(1 + np.exp(-abs(x)))\n",
    "    res = part1 + part2\n",
    "    \n",
    "    return res\n",
    "\n",
    "np.random.seed(123)\n",
    "vec = np.random.normal(size=6)\n",
    "mat = np.random.normal(size=(5, 3))\n",
    "print(\"Sigmoid(v) =\\n\", sigmoid(vec))\n",
    "print(\"Sigmoid(m) =\\n\", sigmoid(mat))\n",
    "print(\"Softplus(v) =\\n\", softplus(vec))\n",
    "print(\"Softplus(m) =\\n\", softplus(mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) 推导 Sigmoid 和 Softplus 函数的导数，编写**数值稳定**的函数 `d_sigmoid(x)` 和 `d_softplus(x)` 计算对应的导数，同样要求适用于向量和矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma(x)=1/(1+\\exp(-x)),\\ \\sigma'(x)=...$\n",
    "\n",
    "$\\mathrm{softplus}(x)=\\log(1+\\exp(x)),\\ \\mathrm{softplus}'(x)=...$\n",
    "\n",
    "注意到 Sigmoid 和 Softplus 之间的联系了吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid'(v) =\n",
      " [0.18871502 0.196853   0.24506124 0.14855047 0.23019077 0.13502129]\n",
      "Sigmoid'(m) =\n",
      " [[0.07457369 0.23884569 0.17157406]\n",
      " [0.20835666 0.223271   0.24944023]\n",
      " [0.14996269 0.22612814 0.23807373]\n",
      " [0.23856977 0.08937477 0.09075383]\n",
      " [0.19624332 0.24090565 0.21887592]]\n",
      "Softplus'(v) =\n",
      " [0.25244196 0.73053634 0.57027629 0.18148857 0.35925474 0.83908511]\n",
      "Softplus'(m) =\n",
      " [[0.08116076 0.39438602 0.78004631]\n",
      " [0.29593301 0.33650995 0.47634044]\n",
      " [0.81628676 0.34549479 0.39079256]\n",
      " [0.39308777 0.90078077 0.89905661]\n",
      " [0.73185488 0.59536432 0.67642017]]\n"
     ]
    }
   ],
   "source": [
    "def d_sigmoid(x):\n",
    "    res = np.zeros_like(x)\n",
    "    res = sigmoid(x) * (1 - sigmoid(x))\n",
    "    return res\n",
    "\n",
    "def d_softplus(x):\n",
    "    res = np.zeros_like(x)\n",
    "    res = sigmoid(x)\n",
    "    return res\n",
    "\n",
    "print(\"Sigmoid'(v) =\\n\", d_sigmoid(vec))\n",
    "print(\"Sigmoid'(m) =\\n\", d_sigmoid(mat))\n",
    "print(\"Softplus'(v) =\\n\", d_softplus(vec))\n",
    "print(\"Softplus'(m) =\\n\", d_softplus(mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) 编写函数 `relu(x)` 和 `d_relu(x)` 来实现 ReLU 函数及其导数，$\\mathrm{ReLU}(x)=\\max(x,0)$，要求适用于向量和矩阵。对于不可导的点，可以将导数取为左导数或右导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU(v) =\n",
      " [0.         0.99734545 0.2829785  0.         0.         1.65143654]\n",
      "ReLU(m) =\n",
      " [[0.         0.         1.26593626]\n",
      " [0.         0.         0.        ]\n",
      " [1.49138963 0.         0.        ]\n",
      " [0.         2.20593008 2.18678609]\n",
      " [1.0040539  0.3861864  0.73736858]]\n",
      "ReLU'(v) =\n",
      " [0 1 1 0 0 1]\n",
      "ReLU'(m) =\n",
      " [[0 0 1]\n",
      " [0 0 0]\n",
      " [1 0 0]\n",
      " [0 1 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    res = np.zeros_like(x)\n",
    "    res = np.where(x > 0, x, 0)\n",
    "    return res\n",
    "\n",
    "def d_relu(x):\n",
    "    res = np.zeros_like(x)\n",
    "    res = np.where(x > 0, 1, 0)\n",
    "    return res\n",
    "\n",
    "print(\"ReLU(v) =\\n\", relu(vec))\n",
    "print(\"ReLU(m) =\\n\", relu(mat))\n",
    "print(\"ReLU'(v) =\\n\", d_relu(vec))\n",
    "print(\"ReLU'(m) =\\n\", d_relu(mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第3题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "练习标量的反向传播算法。考虑一个两层的前馈神经网络（见第5周课件107页），其中数据和权重都是标量：\n",
    "\n",
    "![](img/model1.png)\n",
    "\n",
    "如下为正向的计算过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1.2345\n",
    "w1 = 0.111\n",
    "b1 = 0.222\n",
    "w2 = 0.333\n",
    "b2 = -0.444\n",
    "\n",
    "a0 = x\n",
    "z1 = w1 * a0 + b1\n",
    "a1 = sigmoid(z1)\n",
    "z2 = w2 * a1 + b2\n",
    "a2 = z2\n",
    "\n",
    "yhat = a2\n",
    "y = 5.4321"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令损失函数为 $l=(y-\\hat{y})^2$，则可以计算出 $l$ 对 $a_2=\\hat{y}$ 的导数为 $\\mathrm{d}l/\\mathrm{d}a_2=2(a_2-y)$："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_l_a2 = 2.0 * (a2 - y) # This is the \"upstream derivative\" associated with a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在将 $a_2$ 的导数反向传播给 $z_2$。首先计算 $a_2$ 对 $z_2$ 的“局部导数”：因为 $a_2=z_2$，所以局部导数为1。然后将“上游导数”乘以“局部导数”，即得到“下游导数”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11.360055548261299\n"
     ]
    }
   ],
   "source": [
    "d_a2_z2 = 1.0 # The \"local derivative\" of a2 with respect to z2\n",
    "d_l_z2 = d_l_a2 * d_a2_z2 # The \"downstream derivative\" of l with respect to z2\n",
    "\n",
    "print(d_l_z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来将 $z_2$ 的导数继续向下游传播。在上一层的计算中得到的“下游导数”现在变成了“上游导数”，进而向 $a_1$、$w_2$ 和 $b_2$ 传播。依然进行局部导数的计算，可以得到 $\\mathrm{d}z_2/\\mathrm{d}a_1=w_2$, $\\mathrm{d}z_2/\\mathrm{d}w_2=a_1$, $\\mathrm{d}z_2/\\mathrm{d}b_2=1$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_z2_a1 = w2\n",
    "d_z2_w2 = a1\n",
    "d_z2_b2 = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是通过链式法则，将“上游导数”乘以“局部导数”得到“下游导数”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.7828984975710127\n",
      "-6.688862995036224\n",
      "-11.360055548261299\n"
     ]
    }
   ],
   "source": [
    "d_l_a1 = d_l_z2 * d_z2_a1\n",
    "d_l_w2 = d_l_z2 * d_z2_w2\n",
    "d_l_b2 = d_l_z2 * d_z2_b2\n",
    "\n",
    "print(d_l_a1)\n",
    "print(d_l_w2)\n",
    "print(d_l_b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请完成后续的过程，计算出 $\\mathrm{d}l/\\mathrm{d}w_1$ 和 $\\mathrm{d}l/\\mathrm{d}b_1$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_a1_z1 = a1 * (1 - a1)\n",
    "d_l_z1 = d_l_a1 * d_a1_z1\n",
    "d_z1_w1 = a0\n",
    "d_z1_a0 = w1\n",
    "d_z1_b1 = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1306675893376559\n",
      "-0.9158911213751769\n"
     ]
    }
   ],
   "source": [
    "d_l_w1 = d_l_z1 * d_z1_w1\n",
    "d_l_b1 = d_l_z1 * d_z1_b1\n",
    "print(d_l_w1)\n",
    "print(d_l_b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "d_l_w1 = ...\n",
    "d_l_b1 = ...\n",
    "\n",
    "print(d_l_w1)\n",
    "print(d_l_b1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第4题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在考虑一个更实际的场景，首先，数据包含多个观测，每个观测占据一行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0.0, 0.0],\n",
    "              [0.0, 1.0],\n",
    "              [1.0, 0.0],\n",
    "              [1.0, 1.0]])\n",
    "y = np.array([[0.0],\n",
    "              [1.0],\n",
    "              [1.0],\n",
    "              [0.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出输入的维度为 $p=2$，输出的维度为 $d=1$，样本量为 $n=4$。我们将构建一个两层的前馈神经网络，其中隐藏层的维度为 $r=5$。计算流程如下：\n",
    "\n",
    "![](img/model2.png)\n",
    "\n",
    "$Z_1=XW_1+\\mathbf{1}_nb_1^T,\\quad A_1=\\mathrm{softplus}(Z_1)$\n",
    "\n",
    "$Z_2=A_1W_2+\\mathbf{1}_nb_2^T,\\quad A_2=\\mathrm{sigmoid}(Z_2)$\n",
    "\n",
    "其中 $\\mathbf{1}_n$ 为元素全为1的 $n\\times 1$ 向量，$W_1$ 为 $p\\times r$ 矩阵，$b_1$ 为 $r\\times 1$ 向量，$W_2$ 为 $r\\times d$ 矩阵，$b_2$ 为 $d\\times 1$ 向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99818247]\n",
      " [0.98868386]\n",
      " [0.99961553]\n",
      " [0.99139015]]\n"
     ]
    }
   ],
   "source": [
    "p = 2\n",
    "d = 1\n",
    "n = 4\n",
    "r = 5\n",
    "\n",
    "np.random.seed(123)\n",
    "w1 = np.random.normal(size=(p, r))\n",
    "b1 = np.random.normal(size=(r, 1))\n",
    "w2 = np.random.normal(size=(r, d))\n",
    "b2 = np.random.normal(size=(d, 1))\n",
    "\n",
    "a0 = x\n",
    "ones = np.ones((n, 1))\n",
    "z1 = a0.dot(w1) + ones.dot(b1.T)\n",
    "a1 = softplus(z1)\n",
    "z2 = a1.dot(w2) + ones.dot(b2.T)\n",
    "a2 = sigmoid(z2)\n",
    "\n",
    "phat = a2\n",
    "print(phat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义第 $i$ 个观测的损失函数为 $l(y_i,\\hat{p}_i)=-y_i \\cdot \\log(\\hat{p}_i)-(1-y_i) \\cdot \\log(1-\\hat{p}_i)$，请推导出 $l$ 对 $\\hat{p}_i$ 的导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathrm{d}l/\\mathrm{d}\\hat{p}_i=(\\hat{p}_i-y_i)/(\\hat{p}_i\\cdot(1-\\hat{p}_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义整体的损失函数为 $L(y,\\hat{p})=n^{-1}\\sum_{i=1}^n l(y_i,p_i)$，请给出 $L$ 对 $\\hat{p}_i$ 的导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathrm{d}L/\\mathrm{d}\\hat{p}_i=n^{-1}(\\hat{p}_i-y_i)/(\\hat{p}_i\\cdot(1-\\hat{p}_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 $\\mathrm{d}L/\\mathrm{d}\\hat{p}=(\\mathrm{d}L/\\mathrm{d}\\hat{p}_1,\\ldots,\\mathrm{d}L/\\mathrm{d}\\hat{p}_n)^T$ 看作一个向量，计算出其结果并赋给变量 `d_l_a2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[137.54924583]\n",
      " [ -0.25286142]\n",
      " [ -0.25009615]\n",
      " [ 29.03650883]]\n"
     ]
    }
   ],
   "source": [
    "d_l_a2 = (1 / n) * (phat - y)/(phat * (1 - phat))\n",
    "print(d_l_a2)\n",
    "\n",
    "if d_l_a2.shape != a2.shape:\n",
    "    print(\"Shapes do not match! Please redo the calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "d_l_a2 = ...\n",
    "print(d_l_a2)\n",
    "\n",
    "if d_l_a2.shape != a2.shape:\n",
    "    print(\"Shapes do not match! Please redo the calculation.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`d_l_a2` 是损失函数对 $A_2$ 的“上游导数”，我们需要将其传播给下游 $Z_2$。由于 $A_2=\\mathrm{sigmoid}(Z_2)$，根据课件中反向传播的规则1，可以得到下游导数为\n",
    "\n",
    "$\\mathrm{d}L/\\mathrm{d}Z_2=\\mathrm{d}L/\\mathrm{d}A_2\\circ \\mathrm{sigmoid}'(Z_2)$，\n",
    "\n",
    "其中 $\\mathrm{sigmoid}'$ 是 Sigmoid 的导数，$\\circ$ 是向量或矩阵的逐元素相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.49545617e-01]\n",
      " [-2.82903496e-03]\n",
      " [-9.61178724e-05]\n",
      " [ 2.47847537e-01]]\n"
     ]
    }
   ],
   "source": [
    "d_l_z2 = d_l_a2 * d_sigmoid(z2)\n",
    "print(d_l_z2)\n",
    "\n",
    "if d_l_z2.shape != z2.shape:\n",
    "    print(\"Shapes do not match! Please redo the calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，再将 $Z_2$ 的“上游导数”传给 $A_1$、$W_2$ 和 $b_2$。根据规则2，\n",
    "\n",
    "$\\mathrm{d}L/\\mathrm{d}A_1=(\\mathrm{d}L/\\mathrm{d}Z_2)W_2^T$，\n",
    "\n",
    "$\\mathrm{d}L/\\mathrm{d}W_2=A_1^T(\\mathrm{d}L/\\mathrm{d}Z_2)$，\n",
    "\n",
    "$\\mathrm{d}L/\\mathrm{d}b_2=(\\mathrm{d}L/\\mathrm{d}Z_2)^T\\mathbf{1}_n$。\n",
    "\n",
    "请根据如上公式计算对应的导数，并赋值给相应的变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_A1:\n",
      " [[-1.08390457e-01  5.50480184e-01  5.45702884e-01  2.50557250e-01  9.63711234e-02]\n",
      " [ 1.22879494e-03 -6.24065331e-03 -6.18649429e-03 -2.84050357e-03 -1.09253482e-03]\n",
      " [ 4.17489205e-05 -2.12029306e-04 -2.10189226e-04 -9.65075245e-05 -3.71194151e-05]\n",
      " [-1.07652894e-01  5.46734339e-01  5.41989547e-01  2.48852286e-01  9.57153480e-02]]\n",
      "d_W2:\n",
      " [[0.25687749]\n",
      " [0.20993967]\n",
      " [0.80962486]\n",
      " [0.18883191]\n",
      " [0.15786162]]\n",
      "d_b2:\n",
      " [[0.494468]]\n"
     ]
    }
   ],
   "source": [
    "d_l_a1 = np.matmul(d_l_z2,w2.T)\n",
    "print(\"d_A1:\\n\", d_l_a1)\n",
    "\n",
    "d_l_w2 = np.matmul(a1.T,d_l_z2)\n",
    "print(\"d_W2:\\n\", d_l_w2)\n",
    "\n",
    "tmp2 = np.ones((n,1))\n",
    "d_l_b2 = np.matmul(d_l_z2.T,tmp2)\n",
    "print(\"d_b2:\\n\", d_l_b2)\n",
    "\n",
    "if (d_l_a1.shape != a1.shape) or (d_l_w2.shape != w2.shape) or (d_l_b2.shape != b2.shape):\n",
    "    print(\"Shapes do not match! Please redo the calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "d_l_a1 = ...\n",
    "print(\"d_A1:\\n\", d_l_a1)\n",
    "\n",
    "d_l_w2 = ...\n",
    "print(\"d_W2:\\n\", d_l_w2)\n",
    "\n",
    "d_l_b2 = ...\n",
    "print(\"d_b2:\\n\", d_l_b2)\n",
    "\n",
    "if (d_l_a1.shape != a1.shape) || (d_l_w2.shape != w2.shape) || (d_l_b2.shape != b2.shape):\n",
    "    print(\"Shapes do not match! Please redo the calculation.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请完成后续的过程，计算出 $\\mathrm{d}L/\\mathrm{d}W_1$ 和 $\\mathrm{d}L/\\mathrm{d}b_1$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.42004461e-02  1.37311901e-01  8.18350713e-02  5.66580446e-02  2.29434333e-02]\n",
      " [ 2.44643498e-04 -4.29613316e-04 -1.18082517e-03 -6.44657097e-04 -1.82744670e-04]\n",
      " [ 5.21208714e-06 -4.35235860e-05 -2.60582020e-05 -9.05258758e-06 -7.22163393e-06]\n",
      " [-2.68273708e-02  8.03008290e-02  8.88455563e-02  5.15840795e-02  1.09188555e-02]]\n",
      "d_W1:\n",
      " [[-0.02682216  0.08025731  0.0888195   0.05157503  0.01091163]\n",
      " [-0.02658273  0.07987122  0.08766473  0.05093942  0.01073611]]\n",
      "d_b1:\n",
      " [[-0.05077796]\n",
      " [ 0.21713959]\n",
      " [ 0.16947374]\n",
      " [ 0.10758841]\n",
      " [ 0.03367232]]\n"
     ]
    }
   ],
   "source": [
    "d_l_z1 = d_l_a1 * d_sigmoid(z1)\n",
    "print(d_l_z1)\n",
    "\n",
    "d_l_w1 = np.matmul(a0.T,d_l_z1)\n",
    "print(\"d_W1:\\n\", d_l_w1)\n",
    "\n",
    "tmp2 = np.ones((n, 1))\n",
    "d_l_b1 = np.matmul(d_l_z1.T,tmp2)\n",
    "print(\"d_b1:\\n\", d_l_b1)\n",
    "\n",
    "if (d_l_w1.shape != w1.shape) or (d_l_b1.shape != b1.shape):\n",
    "    print(\"Shapes do not match! Please redo the calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "d_l_w1 = ...\n",
    "print(\"d_W1:\\n\", d_l_w1)\n",
    "\n",
    "d_l_b1 = ...\n",
    "print(\"d_b1:\\n\", d_l_b1)\n",
    "\n",
    "if (d_l_w1.shape != w1.shape) or (d_l_b1.shape != b1.shape):\n",
    "    print(\"Shapes do not match! Please redo the calculation.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第5题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将第4题中的步骤封装成三个函数：`forward()` 计算正向传播结果，得到预测值向量 `phat`，`loss()` 计算整体损失函数值，`backward()` 计算反向传播后各参数的导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, w1, b1, w2, b2):\n",
    "    n = x.shape[0]\n",
    "    ones = np.ones((n, 1))\n",
    "    a0 = x\n",
    "    z1 = a0.dot(w1) + ones.dot(b1.T)\n",
    "    a1 = softplus(z1)\n",
    "    z2 = a1.dot(w2) + ones.dot(b2.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    return z1, a1, z2, a2\n",
    "\n",
    "def loss_function(y, phat):\n",
    "    loss1 = -y * np.log(phat) - (1 - y) * np.log(1 - phat)\n",
    "    loss = np.sum(loss1)\n",
    "    return loss\n",
    "\n",
    "def backward(x, w1, b1, w2, b2, z1, a1, z2, a2, y):\n",
    "    a0 = x\n",
    "    d_l_a2 = (1 / n) * (phat - y)/(phat * (1 - phat))\n",
    "    d_l_z2 = d_l_a2 * d_sigmoid(z2)\n",
    "    d_l_a1 = np.matmul(d_l_z2,w2.T)\n",
    "    d_l_w2 = np.matmul(a1.T,d_l_z2)\n",
    "    tmp2 = np.ones((n,1))\n",
    "    d_l_b2 = np.matmul(d_l_z2.T,tmp2)\n",
    "    d_l_z1 = d_l_a1 * d_sigmoid(z1)\n",
    "    d_l_w1 = np.matmul(a0.T,d_l_z1)\n",
    "    tmp1 = np.ones((n,1))\n",
    "    d_l_b1 = np.matmul(d_l_z1.T,tmp1)\n",
    "    return d_l_w1, d_l_b1, d_l_w2, d_l_b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与之前的结果互相印证："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phat =\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "loss = nan\n",
      "(dw1, db1, dw2, db2) =\n",
      " (array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan]]), array([[nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan]]), array([[nan]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-91ee125e3134>:12: RuntimeWarning: divide by zero encountered in log\n",
      "  loss1 = -y * np.log(phat) - (1 - y) * np.log(1 - phat)\n",
      "<ipython-input-19-91ee125e3134>:12: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss1 = -y * np.log(phat) - (1 - y) * np.log(1 - phat)\n",
      "<ipython-input-19-91ee125e3134>:18: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  d_l_a2 = (1 / n) * (phat - y)/(phat * (1 - phat))\n",
      "<ipython-input-19-91ee125e3134>:18: RuntimeWarning: invalid value encountered in true_divide\n",
      "  d_l_a2 = (1 / n) * (phat - y)/(phat * (1 - phat))\n",
      "<ipython-input-19-91ee125e3134>:19: RuntimeWarning: invalid value encountered in multiply\n",
      "  d_l_z2 = d_l_a2 * d_sigmoid(z2)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.0, 0.0],\n",
    "              [0.0, 1.0],\n",
    "              [1.0, 0.0],\n",
    "              [1.0, 1.0]])\n",
    "y = np.array([[0.0],\n",
    "              [1.0],\n",
    "              [1.0],\n",
    "              [0.0]])\n",
    "p = 2\n",
    "d = 1\n",
    "n = 4\n",
    "r = 5\n",
    "\n",
    "np.random.seed(123)\n",
    "w1 = np.random.normal(size=(p, r))\n",
    "b1 = np.random.normal(size=(r, 1))\n",
    "w2 = np.random.normal(size=(r, d))\n",
    "b2 = np.random.normal(size=(d, 1))\n",
    "\n",
    "z1, a1, z2, a2 = forward(x, w1, b1, w2, b2)\n",
    "phat = a2\n",
    "loss = loss_function(y, phat)\n",
    "grads = backward(x, w1, b1, w2, b2, z1, a1, z2, a2, y)\n",
    "\n",
    "print(\"phat =\\n\", phat)\n",
    "print(\"loss =\", loss)\n",
    "print(\"(dw1, db1, dw2, db2) =\\n\", grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后将这三步放在循环中，并利用梯度下降法拟合出 $x$ 和 $y$ 之间的函数关系。下面是一个完整的神经网络模型，我们可以修改隐藏层的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss = 9.994059588113242, prediction = [0.99877267 0.99287977 0.99088107 0.9621769 ]\n",
      "iteration 50, loss = 3.040508840339436, prediction = [0.75977761 0.53392016 0.52247324 0.2865395 ]\n",
      "iteration 100, loss = 2.5658279196891334, prediction = [0.56969021 0.57861744 0.50292919 0.38624436]\n",
      "iteration 150, loss = 2.2353010215891342, prediction = [0.48124296 0.62825427 0.53184384 0.38292563]\n",
      "iteration 200, loss = 1.9325028107737185, prediction = [0.42212657 0.66897135 0.5790532  0.353206  ]\n",
      "iteration 250, loss = 1.6577155743628067, prediction = [0.37134893 0.70464389 0.63052275 0.31768776]\n",
      "iteration 300, loss = 1.4149325528010934, prediction = [0.32502805 0.73728421 0.68002526 0.28211167]\n",
      "iteration 350, loss = 1.205726247301902, prediction = [0.28310063 0.7671249  0.72482555 0.24871976]\n",
      "iteration 400, loss = 1.029064230500413, prediction = [0.2459945  0.79391989 0.76389922 0.21855966]\n",
      "iteration 450, loss = 0.881985083858663, prediction = [0.21383666 0.81751794 0.79717329 0.19202871]\n",
      "iteration 500, loss = 0.7605336375196772, prediction = [0.18638725 0.83799023 0.82509304 0.16910755]\n",
      "iteration 550, loss = 0.6605512370867943, prediction = [0.16317014 0.8555836  0.84833476 0.14952929]\n",
      "iteration 600, loss = 0.5781684100030573, prediction = [0.14361348 0.8706361  0.86762652 0.13290768]\n",
      "iteration 650, loss = 0.5100319103270488, prediction = [0.1271463  0.88350826 0.8836515  0.11882358]\n",
      "iteration 700, loss = 0.4533581801288455, prediction = [0.11324949 0.89454106 0.89700626 0.10687583]\n",
      "iteration 750, loss = 0.4058956183419511, prediction = [0.10147547 0.90403611 0.90819066 0.09670621]\n",
      "iteration 800, loss = 0.365849807097138, prediction = [0.09145001 0.91224986 0.91761341 0.08800793]\n",
      "iteration 850, loss = 0.33180141861519097, prediction = [0.08286569 0.91939547 0.92560414 0.08052509]\n",
      "iteration 900, loss = 0.30263066325412835, prediction = [0.0754723  0.92564794 0.93242672 0.07404753]\n",
      "iteration 950, loss = 0.27745329153549925, prediction = [0.06906705 0.93115016 0.93829183 0.06840423]\n",
      "iteration 1000, loss = 0.25556878399862665, prediction = [0.06348566 0.93601867 0.9433677  0.06345654]\n",
      "iteration 1050, loss = 0.2364194887619221, prediction = [0.05859483 0.94034879 0.94778903 0.05909214]\n",
      "iteration 1100, loss = 0.21955887209260605, prediction = [0.05428606 0.9442187  0.95166403 0.05521986]\n",
      "iteration 1150, loss = 0.20462704475901208, prediction = [0.05047065 0.94769293 0.95508008 0.05176544]\n",
      "iteration 1200, loss = 0.1913319587769692, prediction = [0.04707575 0.95082503 0.95810813 0.04866806]\n",
      "iteration 1250, loss = 0.17943496234448394, prediction = [0.04404122 0.95365968 0.96080609 0.04587768]\n",
      "iteration 1300, loss = 0.16873967954145405, prediction = [0.04131715 0.95623442 0.96322155 0.04335288]\n",
      "iteration 1350, loss = 0.15908341839549306, prediction = [0.0388619  0.95858096 0.96539384 0.04105911]\n",
      "iteration 1400, loss = 0.15033050124371075, prediction = [0.03664054 0.96072625 0.96735564 0.03896744]\n",
      "iteration 1450, loss = 0.14236705927315665, prediction = [0.03462362 0.96269331 0.9691343  0.03705345]\n",
      "iteration 1500, loss = 0.13509694595481977, prediction = [0.03278619 0.96450191 0.97075283 0.03529641]\n",
      "iteration 1550, loss = 0.12843850918146724, prediction = [0.03110699 0.96616911 0.97223067 0.03367865]\n",
      "iteration 1600, loss = 0.12232202571356957, prediction = [0.02956784 0.96770969 0.97358438 0.03218499]\n",
      "iteration 1650, loss = 0.11668764924227495, prediction = [0.02815312 0.96913654 0.97482809 0.03080235]\n",
      "iteration 1700, loss = 0.11148375904821071, prediction = [0.02684934 0.97046091 0.97597394 0.02951938]\n",
      "iteration 1750, loss = 0.10666562294907508, prediction = [0.0256448  0.97169268 0.97703243 0.0283262 ]\n",
      "iteration 1800, loss = 0.10219430829292864, prediction = [0.02452934 0.97284055 0.97801262 0.02721416]\n",
      "iteration 1850, loss = 0.09803578987952746, prediction = [0.02349406 0.9739122  0.97892244 0.02617568]\n",
      "iteration 1900, loss = 0.09416021514409231, prediction = [0.02253118 0.97491444 0.9797688  0.02520405]\n",
      "iteration 1950, loss = 0.09054129565031359, prediction = [0.02163383 0.97585335 0.98055775 0.02429337]\n",
      "iteration 2000, loss = 0.0871558006012156, prediction = [0.02079598 0.97673431 0.98129463 0.02343836]\n",
      "iteration 2050, loss = 0.08398313319723776, prediction = [0.02001226 0.97756217 0.98198416 0.02263434]\n",
      "iteration 2100, loss = 0.08100497462817065, prediction = [0.01927792 0.97834124 0.98263053 0.02187712]\n",
      "iteration 2150, loss = 0.07820498356027204, prediction = [0.01858872 0.97907543 0.98323744 0.02116295]\n",
      "iteration 2200, loss = 0.07556854138174086, prediction = [0.01794087 0.97976824 0.98380822 0.02048846]\n",
      "iteration 2250, loss = 0.07308253535594836, prediction = [0.01733099 0.98042284 0.98434582 0.0198506 ]\n",
      "iteration 2300, loss = 0.07073517332094437, prediction = [0.01675604 0.98104208 0.98485291 0.01924664]\n",
      "iteration 2350, loss = 0.06851582475540147, prediction = [0.01621327 0.98162857 0.98533188 0.01867408]\n",
      "iteration 2400, loss = 0.06641488397355902, prediction = [0.01570023 0.98218466 0.98578488 0.01813067]\n",
      "iteration 2450, loss = 0.06442365196698865, prediction = [0.01521467 0.98271251 0.98621387 0.01761437]\n",
      "iteration 2500, loss = 0.06253423401918598, prediction = [0.01475458 0.98321407 0.98662059 0.01712329]\n",
      "iteration 2550, loss = 0.060739450711004034, prediction = [0.01431812 0.98369113 0.98700665 0.01665575]\n",
      "iteration 2600, loss = 0.059032760334729, prediction = [0.01390362 0.98414532 0.98737351 0.01621017]\n",
      "iteration 2650, loss = 0.05740819106089501, prediction = [0.01350956 0.98457814 0.98772249 0.01578512]\n",
      "iteration 2700, loss = 0.05586028146933299, prediction = [0.01313455 0.98499098 0.9880548  0.01537931]\n",
      "iteration 2750, loss = 0.05438402827598026, prediction = [0.01277732 0.9853851  0.98837154 0.01499152]\n",
      "iteration 2800, loss = 0.05297484026872494, prediction = [0.01243671 0.98576165 0.98867373 0.01462063]\n",
      "iteration 2850, loss = 0.051628497616291076, prediction = [0.01211165 0.98612173 0.9889623  0.01426564]\n",
      "iteration 2900, loss = 0.05034111583956361, prediction = [0.01180115 0.98646631 0.98923811 0.01392558]\n",
      "iteration 2950, loss = 0.04910911383951293, prediction = [0.01150432 0.98679632 0.98950193 0.0135996 ]\n",
      "iteration 3000, loss = 0.047929185463623625, prediction = [0.01122033 0.98711261 0.98975449 0.01328687]\n",
      "iteration 3050, loss = 0.046798274166533406, prediction = [0.01094839 0.98741595 0.98999647 0.01298665]\n",
      "iteration 3100, loss = 0.045713550382783685, prediction = [0.01068781 0.98770709 0.99022849 0.01269824]\n",
      "iteration 3150, loss = 0.04467239128223937, prediction = [0.01043792 0.9879867  0.9904511  0.012421  ]\n",
      "iteration 3200, loss = 0.04367236262337472, prediction = [0.01019812 0.9882554  0.99066486 0.01215432]\n",
      "iteration 3250, loss = 0.04271120245760854, prediction = [0.00996784 0.9885138  0.99087024 0.01189764]\n",
      "iteration 3300, loss = 0.04178680647028788, prediction = [0.00974655 0.98876243 0.99106772 0.01165044]\n",
      "iteration 3350, loss = 0.04089721477163889, prediction = [0.00953377 0.9890018  0.9912577  0.01141223]\n",
      "iteration 3400, loss = 0.040040599974774854, prediction = [0.00932904 0.9892324  0.9914406  0.01118255]\n",
      "iteration 3450, loss = 0.03921525641828703, prediction = [0.00913194 0.98945467 0.99161678 0.01096097]\n",
      "iteration 3500, loss = 0.03841959040856717, prediction = [0.00894207 0.98966903 0.99178659 0.0107471 ]\n",
      "iteration 3550, loss = 0.03765211137220629, prediction = [0.00875906 0.98987586 0.99195035 0.01054056]\n",
      "iteration 3600, loss = 0.03691142382199927, prediction = [0.00858256 0.99007554 0.99210836 0.010341  ]\n",
      "iteration 3650, loss = 0.036196220051505495, prediction = [0.00841226 0.99026841 0.99226091 0.01014809]\n",
      "iteration 3700, loss = 0.03550527348304475, prediction = [0.00824784 0.99045479 0.99240826 0.00996152]\n",
      "iteration 3750, loss = 0.034837432602669804, prediction = [0.00808903 0.99063498 0.99255066 0.00978099]\n",
      "iteration 3800, loss = 0.03419161542321161, prediction = [0.00793555 0.99080927 0.99268834 0.00960623]\n",
      "iteration 3850, loss = 0.03356680442309193, prediction = [0.00778716 0.99097794 0.99282153 0.00943699]\n",
      "iteration 3900, loss = 0.032962041914399755, prediction = [0.00764362 0.99114123 0.99295043 0.00927301]\n",
      "iteration 3950, loss = 0.0323764257988082, prediction = [0.00750471 0.99129938 0.99307523 0.00911408]\n",
      "iteration 4000, loss = 0.03180910567436811, prediction = [0.00737021 0.99145262 0.99319612 0.00895996]\n",
      "iteration 4050, loss = 0.03125927926016997, prediction = [0.00723994 0.99160115 0.99331326 0.00881046]\n",
      "iteration 4100, loss = 0.03072618910932745, prediction = [0.0071137  0.9917452  0.99342684 0.00866537]\n",
      "iteration 4150, loss = 0.030209119583810843, prediction = [0.00699133 0.99188493 0.99353698 0.00852453]\n",
      "iteration 4200, loss = 0.029707394067381307, prediction = [0.00687265 0.99202054 0.99364385 0.00838774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4250, loss = 0.029220372395279567, prediction = [0.00675751 0.99215218 0.99374759 0.00825486]\n",
      "iteration 4300, loss = 0.02874744848147441, prediction = [0.00664576 0.99228004 0.99384831 0.00812571]\n",
      "iteration 4350, loss = 0.028288048126165748, prediction = [0.00653726 0.99240425 0.99394614 0.00800016]\n",
      "iteration 4400, loss = 0.027841626987946413, prediction = [0.00643187 0.99252496 0.99404121 0.00787805]\n",
      "iteration 4450, loss = 0.02740766870652838, prediction = [0.00632948 0.99264232 0.99413361 0.00775927]\n",
      "iteration 4500, loss = 0.026985683163293996, prediction = [0.00622996 0.99275645 0.99422347 0.00764367]\n",
      "iteration 4550, loss = 0.02657520486813811, prediction = [0.0061332  0.99286747 0.99431086 0.00753115]\n",
      "iteration 4600, loss = 0.026175791462146634, prediction = [0.00603909 0.99297551 0.9943959  0.00742158]\n",
      "iteration 4650, loss = 0.02578702232663313, prediction = [0.00594753 0.99308068 0.99447867 0.00731485]\n",
      "iteration 4700, loss = 0.02540849728991067, prediction = [0.00585842 0.99318308 0.99455926 0.00721087]\n",
      "iteration 4750, loss = 0.02503983542397522, prediction = [0.00577167 0.99328282 0.99463774 0.00710952]\n",
      "iteration 4800, loss = 0.024680673923962664, prediction = [0.00568719 0.99337999 0.9947142  0.00701072]\n",
      "iteration 4850, loss = 0.02433066706389155, prediction = [0.00560489 0.99347469 0.99478871 0.00691438]\n",
      "iteration 4900, loss = 0.023989485222759357, prediction = [0.00552471 0.993567   0.99486134 0.00682041]\n",
      "iteration 4950, loss = 0.02365681397558747, prediction = [0.00544656 0.99365701 0.99493216 0.00672872]\n",
      "iteration 5000, loss = 0.02333235324447311, prediction = [0.00537036 0.99374481 0.99500123 0.00663924]\n",
      "iteration 5050, loss = 0.023015816505121613, prediction = [0.00529605 0.99383046 0.99506861 0.00655189]\n",
      "iteration 5100, loss = 0.022706930044724, prediction = [0.00522357 0.99391404 0.99513436 0.0064666 ]\n",
      "iteration 5150, loss = 0.02240543226738589, prediction = [0.00515285 0.99399563 0.99519854 0.00638331]\n",
      "iteration 5200, loss = 0.02211107304362863, prediction = [0.00508382 0.99407528 0.9952612  0.00630194]\n",
      "iteration 5250, loss = 0.021823613100768163, prediction = [0.00501644 0.99415307 0.99532239 0.00622243]\n",
      "iteration 5300, loss = 0.02154282345123807, prediction = [0.00495065 0.99422905 0.99538216 0.00614473]\n",
      "iteration 5350, loss = 0.021268484856160688, prediction = [0.00488639 0.99430329 0.99544056 0.00606876]\n",
      "iteration 5400, loss = 0.02100038732168135, prediction = [0.00482361 0.99437583 0.99549763 0.00599449]\n",
      "iteration 5450, loss = 0.02073832962578132, prediction = [0.00476227 0.99444675 0.99555342 0.00592185]\n",
      "iteration 5500, loss = 0.02048211887345956, prediction = [0.00470231 0.99451607 0.99560796 0.0058508 ]\n",
      "iteration 5550, loss = 0.020231570078342885, prediction = [0.0046437  0.99458387 0.99566129 0.00578128]\n",
      "iteration 5600, loss = 0.019986505768926226, prediction = [0.00458639 0.99465018 0.99571346 0.00571325]\n",
      "iteration 5650, loss = 0.019746755617789012, prediction = [0.00453034 0.99471505 0.9957645  0.00564666]\n",
      "iteration 5700, loss = 0.019512156092252098, prediction = [0.00447551 0.99477852 0.99581445 0.00558147]\n",
      "iteration 5750, loss = 0.019282550125059122, prediction = [0.00442187 0.99484065 0.99586333 0.00551764]\n",
      "iteration 5800, loss = 0.019057786803771463, prediction = [0.00436937 0.99490146 0.99591118 0.00545512]\n",
      "iteration 5850, loss = 0.018837721077654974, prediction = [0.00431799 0.99496099 0.99595804 0.00539389]\n",
      "iteration 5900, loss = 0.018622213480941653, prediction = [0.00426768 0.99501929 0.99600392 0.00533389]\n",
      "iteration 5950, loss = 0.018411129871407465, prediction = [0.00421842 0.9950764  0.99604886 0.0052751 ]\n",
      "iteration 6000, loss = 0.018204341183307805, prediction = [0.00417018 0.99513234 0.99609289 0.00521748]\n",
      "iteration 6050, loss = 0.018001723193762236, prediction = [0.00412292 0.99518715 0.99613604 0.005161  ]\n",
      "iteration 6100, loss = 0.01780315630175239, prediction = [0.00407662 0.99524086 0.99617832 0.00510562]\n",
      "iteration 6150, loss = 0.01760852531895292, prediction = [0.00403125 0.9952935  0.99621976 0.00505132]\n",
      "iteration 6200, loss = 0.017417719271671808, prediction = [0.00398678 0.99534511 0.9962604  0.00499806]\n",
      "iteration 6250, loss = 0.01723063121322545, prediction = [0.00394319 0.99539571 0.99630024 0.00494582]\n",
      "iteration 6300, loss = 0.017047158046114372, prediction = [0.00390046 0.99544532 0.99633931 0.00489457]\n",
      "iteration 6350, loss = 0.016867200353420105, prediction = [0.00385855 0.99549399 0.99637764 0.00484428]\n",
      "iteration 6400, loss = 0.016690662238871147, prediction = [0.00381745 0.99554173 0.99641524 0.00479493]\n",
      "iteration 6450, loss = 0.01651745117506934, prediction = [0.00377714 0.99558856 0.99645213 0.00474648]\n",
      "iteration 6500, loss = 0.01634747785939869, prediction = [0.00373759 0.99563452 0.99648833 0.00469893]\n",
      "iteration 6550, loss = 0.01618065607717474, prediction = [0.00369878 0.99567963 0.99652386 0.00465224]\n",
      "iteration 6600, loss = 0.016016902571612264, prediction = [0.00366069 0.9957239  0.99655875 0.00460639]\n",
      "iteration 6650, loss = 0.015856136920226097, prediction = [0.00362331 0.99576736 0.99659299 0.00456136]\n",
      "iteration 6700, loss = 0.0156982814173006, prediction = [0.00358662 0.99581003 0.99662662 0.00451713]\n",
      "iteration 6750, loss = 0.015543260962083442, prediction = [0.00355059 0.99585194 0.99665964 0.00447368]\n",
      "iteration 6800, loss = 0.015391002952387628, prediction = [0.00351521 0.99589309 0.99669208 0.00443099]\n",
      "iteration 6850, loss = 0.015241437183300417, prediction = [0.00348046 0.99593351 0.99672395 0.00438904]\n",
      "iteration 6900, loss = 0.01509449575071661, prediction = [0.00344633 0.99597323 0.99675526 0.00434781]\n",
      "iteration 6950, loss = 0.014950112959436814, prediction = [0.00341281 0.99601224 0.99678602 0.00430728]\n",
      "iteration 7000, loss = 0.014808225235578175, prediction = [0.00337987 0.99605059 0.99681625 0.00426744]\n",
      "iteration 7050, loss = 0.014668771043072048, prediction = [0.0033475  0.99608827 0.99684597 0.00422827]\n",
      "iteration 7100, loss = 0.014531690804022258, prediction = [0.00331569 0.9961253  0.99687518 0.00418976]\n",
      "iteration 7150, loss = 0.014396926822724806, prediction = [0.00328442 0.99616171 0.9969039  0.00415188]\n",
      "iteration 7200, loss = 0.01426442321315654, prediction = [0.00325369 0.99619751 0.99693214 0.00411463]\n",
      "iteration 7250, loss = 0.014134125829744522, prediction = [0.00322347 0.99623271 0.99695991 0.00407799]\n",
      "iteration 7300, loss = 0.014005982201255568, prediction = [0.00319376 0.99626732 0.99698722 0.00404194]\n",
      "iteration 7350, loss = 0.01387994146763533, prediction = [0.00316454 0.99630137 0.99701409 0.00400647]\n",
      "iteration 7400, loss = 0.013755954319654044, prediction = [0.0031358  0.99633485 0.99704052 0.00397157]\n",
      "iteration 7450, loss = 0.013633972941207747, prediction = [0.00310754 0.99636779 0.99706652 0.00393722]\n",
      "iteration 7500, loss = 0.013513950954150332, prediction = [0.00307973 0.99640021 0.99709211 0.00390341]\n",
      "iteration 7550, loss = 0.013395843365519305, prediction = [0.00305237 0.9964321  0.99711729 0.00387014]\n",
      "iteration 7600, loss = 0.013279606517043682, prediction = [0.00302545 0.99646348 0.99714207 0.00383738]\n",
      "iteration 7650, loss = 0.013165198036817963, prediction = [0.00299896 0.99649437 0.99716646 0.00380513]\n",
      "iteration 7700, loss = 0.013052576793034707, prediction = [0.00297289 0.99652477 0.99719048 0.00377337]\n",
      "iteration 7750, loss = 0.012941702849674573, prediction = [0.00294722 0.9965547  0.99721412 0.0037421 ]\n",
      "iteration 7800, loss = 0.012832537424062762, prediction = [0.00292196 0.99658417 0.9972374  0.00371129]\n",
      "iteration 7850, loss = 0.012725042846198504, prediction = [0.00289709 0.99661318 0.99726032 0.00368096]\n",
      "iteration 7900, loss = 0.012619182519770913, prediction = [0.0028726  0.99664175 0.9972829  0.00365107]\n",
      "iteration 7950, loss = 0.01251492088478512, prediction = [0.00284848 0.99666989 0.99730513 0.00362163]\n",
      "iteration 8000, loss = 0.012412223381720554, prediction = [0.00282473 0.9966976  0.99732704 0.00359262]\n",
      "iteration 8050, loss = 0.012311056417147016, prediction = [0.00280134 0.9967249  0.99734861 0.00356404]\n",
      "iteration 8100, loss = 0.012211387330734277, prediction = [0.0027783  0.99675179 0.99736987 0.00353588]\n",
      "iteration 8150, loss = 0.01211318436358877, prediction = [0.0027556  0.99677829 0.99739082 0.00350812]\n",
      "iteration 8200, loss = 0.01201641662785358, prediction = [0.00273323 0.99680439 0.99741147 0.00348075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8250, loss = 0.011921054077518497, prediction = [0.0027112  0.99683012 0.99743181 0.00345378]\n",
      "iteration 8300, loss = 0.011827067480382335, prediction = [0.00268949 0.99685547 0.99745186 0.0034272 ]\n",
      "iteration 8350, loss = 0.011734428391112954, prediction = [0.00266809 0.99688045 0.99747163 0.00340098]\n",
      "iteration 8400, loss = 0.011643109125360712, prediction = [0.002647   0.99690508 0.99749111 0.00337514]\n",
      "iteration 8450, loss = 0.01155308273487561, prediction = [0.00262621 0.99692936 0.99751032 0.00334965]\n",
      "iteration 8500, loss = 0.011464322983579333, prediction = [0.00260571 0.99695329 0.99752926 0.00332452]\n",
      "iteration 8550, loss = 0.011376804324559533, prediction = [0.00258551 0.99697689 0.99754794 0.00329973]\n",
      "iteration 8600, loss = 0.011290501877935583, prediction = [0.00256559 0.99700016 0.99756636 0.00327528]\n",
      "iteration 8650, loss = 0.01120539140956345, prediction = [0.00254595 0.9970231  0.99758452 0.00325116]\n",
      "iteration 8700, loss = 0.011121449310544165, prediction = [0.00252658 0.99704573 0.99760244 0.00322737]\n",
      "iteration 8750, loss = 0.011038652577494748, prediction = [0.00250748 0.99706805 0.99762011 0.0032039 ]\n",
      "iteration 8800, loss = 0.010956978793556577, prediction = [0.00248864 0.99709006 0.99763755 0.00318074]\n",
      "iteration 8850, loss = 0.010876406110104325, prediction = [0.00247006 0.99711177 0.99765475 0.00315788]\n",
      "iteration 8900, loss = 0.010796913229127134, prediction = [0.00245173 0.9971332  0.99767172 0.00313533]\n",
      "iteration 8950, loss = 0.010718479386256391, prediction = [0.00243364 0.99715433 0.99768846 0.00311308]\n",
      "iteration 9000, loss = 0.010641084334406019, prediction = [0.0024158  0.99717518 0.99770499 0.00309111]\n",
      "iteration 9050, loss = 0.010564708328009272, prediction = [0.0023982  0.99719576 0.99772129 0.00306943]\n",
      "iteration 9100, loss = 0.010489332107817748, prediction = [0.00238083 0.99721607 0.99773739 0.00304802]\n",
      "iteration 9150, loss = 0.010414936886246225, prediction = [0.00236368 0.99723611 0.99775328 0.0030269 ]\n",
      "iteration 9200, loss = 0.01034150433323822, prediction = [0.00234676 0.99725589 0.99776896 0.00300604]\n",
      "iteration 9250, loss = 0.010269016562629747, prediction = [0.00233006 0.99727541 0.99778444 0.00298544]\n",
      "iteration 9300, loss = 0.010197456118994214, prediction = [0.00231358 0.99729468 0.99779972 0.00296511]\n",
      "iteration 9350, loss = 0.010126805964946346, prediction = [0.00229731 0.99731371 0.99781481 0.00294502]\n",
      "iteration 9400, loss = 0.01005704946888779, prediction = [0.00228125 0.99733249 0.99782971 0.00292519]\n",
      "iteration 9450, loss = 0.009988170393175568, prediction = [0.00226539 0.99735104 0.99784442 0.00290561]\n",
      "iteration 9500, loss = 0.009920152882700022, prediction = [0.00224973 0.99736935 0.99785895 0.00288626]\n",
      "iteration 9550, loss = 0.009852981453850985, prediction = [0.00223427 0.99738744 0.9978733  0.00286715]\n",
      "iteration 9600, loss = 0.009786640983859883, prediction = [0.002219   0.9974053  0.99788747 0.00284828]\n",
      "iteration 9650, loss = 0.009721116700505163, prediction = [0.00220392 0.99742293 0.99790147 0.00282963]\n",
      "iteration 9700, loss = 0.009656394172160408, prediction = [0.00218903 0.99744035 0.9979153  0.00281121]\n",
      "iteration 9750, loss = 0.009592459298180048, prediction = [0.00217432 0.99745756 0.99792896 0.002793  ]\n",
      "iteration 9800, loss = 0.009529298299600588, prediction = [0.00215979 0.99747456 0.99794246 0.00277502]\n",
      "iteration 9850, loss = 0.00946689771015568, prediction = [0.00214543 0.99749135 0.99795579 0.00275725]\n",
      "iteration 9900, loss = 0.009405244367582794, prediction = [0.00213126 0.99750794 0.99796897 0.00273969]\n",
      "iteration 9950, loss = 0.00934432540521726, prediction = [0.00211725 0.99752433 0.99798199 0.00272233]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.0, 0.0],\n",
    "              [0.0, 1.0],\n",
    "              [1.0, 0.0],\n",
    "              [1.0, 1.0]])\n",
    "y = np.array([[0.0],\n",
    "              [1.0],\n",
    "              [1.0],\n",
    "              [0.0]])\n",
    "\n",
    "n = x.shape[0]\n",
    "p = x.shape[1]\n",
    "d = y.shape[1]\n",
    "r = 15\n",
    "\n",
    "np.random.seed(123)\n",
    "w1 = np.random.normal(size=(p, r))\n",
    "b1 = np.random.normal(size=(r, 1))\n",
    "w2 = np.random.normal(size=(r, d))\n",
    "b2 = np.random.normal(size=(d, 1))\n",
    "\n",
    "nepoch = 10000\n",
    "learning_rate = 0.1\n",
    "\n",
    "for i in range(nepoch):\n",
    "    z1, a1, z2, a2 = forward(x, w1, b1, w2, b2)\n",
    "    phat = a2\n",
    "    loss = loss_function(y, phat)\n",
    "    dw1, db1, dw2, db2 = backward(x, w1, b1, w2, b2, z1, a1, z2, a2, y)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(f\"iteration {i}, loss = {loss}, prediction = {phat.squeeze()}\")\n",
    "        \n",
    "    w1 = w1 - learning_rate * dw1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    w2 = w2 - learning_rate * dw2\n",
    "    b2 = b2 - learning_rate * db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
